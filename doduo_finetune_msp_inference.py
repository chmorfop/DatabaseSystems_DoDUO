# -*- coding: utf-8 -*-
"""DoDUO_Finetune_MSP_Inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eaCb6e0RT9WqYEhoqFjnZucOnXYsNK3o

### Clone the DoDUO repository, fetch the data , install the packages
"""

!git clone https://github.com/megagonlabs/doduo

# Commented out IPython magic to ensure Python compatibility.
# %cd doduo

!bash download.sh

!pip install numpy -q
!pip install pandas -q
!pip install scikit-learn -q
!pip install scipy -q
!pip install torch -q
!pip install transformers -q

"""### Masked Column Prediction"""

from transformers import BertTokenizer, BertForMaskedLM
import torch
from transformers import Trainer, TrainingArguments
from transformers import AutoTokenizer
from transformers import BertTokenizer
from transformers import DataCollatorForLanguageModeling
from torch.utils.data.dataset import Dataset
import random
from transformers import AdamW
from tqdm import tqdm  # for our progress bar
import pickle
import math
import matplotlib.pyplot as plt
import numpy as np

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')

"""##### Custom Functions"""

def modify_df_list(df):
  '''
  return list with grouped values with respect to a df
  '''
  row_list = []
  new_row_list = []
  for (index, group_df) in df.groupby('table_id'):

      row_list.append(group_df.data.values.tolist() )

  for r in row_list:
    new_row_list.append(' | '.join(r))
  return new_row_list

def calculate_perplexity(avg_train_loss,avg_val_loss):
  '''
  calculate perplexity with respect to loss
  '''
  train_perplexity_list= []
  val_perplexity_list = []
  for i,t in enumerate(avg_train_loss):
    train_perplexity_list.append(math.exp(avg_train_loss[i]))
    val_perplexity_list.append(math.exp(avg_val_loss[i]))
  return train_perplexity_list , val_perplexity_list

# loading the data
with open('/content/doduo/data/table_col_type_serialized.pkl', 'rb') as f:
    all_data = pickle.load(f)

# Defining the threshold of training data
threshold = 250000

train_df = all_data.get('train').head(threshold)
val_df = all_data.get('dev')

train_df_list = modify_df_list(train_df)
val_df_list = modify_df_list(val_df)

class Custom_MCP_Dataset(Dataset):
    def __init__(self,examples):
        self.examples = examples
        self.selections = random.sample(range(0,len(examples)), int(0.2*len(examples)) )
        
    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        res = tokenizer(self.examples[i], return_tensors='pt', max_length=150, truncation=True, padding='max_length')
        res.input_ids = torch.where(res.input_ids == 1064, 102, res.input_ids)
        res['labels'] = res.input_ids.detach().clone()
        if i in self.selections:
          # find the positions of the [SEP] tokens
          sep_positions = torch.where(res.input_ids == tokenizer.sep_token_id)[1]
          # create a new tensor of the same shape as the input tensor
          masked_tensor = res.input_ids.clone()
          # replace the values between the [SEP] tokens with the MASK token
          for i in range(sep_positions.shape[0]-1):
              start_idx = sep_positions[i] + 1
              end_idx = sep_positions[i+1]
              masked_tensor[0, start_idx:end_idx] = tokenizer.mask_token_id
              res.input_ids = masked_tensor

        else:
          pass

        res.input_ids = res.input_ids.squeeze()
        res.labels = res.labels.squeeze()
        res.token_type_ids = res.token_type_ids.squeeze()
        res.attention_mask = res.attention_mask.squeeze()

        return {
        'input_ids':res.input_ids.squeeze(),  
        'token_type_ids':res.token_type_ids.squeeze(),  
        'attention_mask':res.attention_mask.squeeze(),  
        'labels':res.labels.squeeze(),  

        }

"""##### Example Dataset"""

train_df[train_df.table_id.str.contains('10535445-4')]

example_MSP_dataset = Custom_MCP_Dataset(train_df_list[:10000])
example_MSP_loader = torch.utils.data.DataLoader(example_MSP_dataset, batch_size=64, shuffle=True)

tokenizer.decode(example_MSP_dataset[7547].get('input_ids'))

tokenizer.decode(example_MSP_dataset[7547].get('labels'))

tokenizer.decode(example_MSP_dataset[2350].get('input_ids'))

tokenizer.decode(example_MSP_dataset[2350].get('labels'))

"""#### Training via MCP"""

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

optim = AdamW(model.parameters(), lr=5e-5)

val_MCP_dataset = Custom_MCP_Dataset(val_df_list)
val_MCP_loader = torch.utils.data.DataLoader(val_MCP_dataset, batch_size=64, shuffle=True)

epochs = 6
avg_train_loss = []
avg_val_loss = []

for epoch in range(epochs):
    train_loss = 0
    val_loss = 0

    train_MCP_dataset = Custom_MCP_Dataset(train_df_list)
    train_MCP_loader = torch.utils.data.DataLoader(train_MCP_dataset, batch_size=64, shuffle=True)

    loop = tqdm(train_MCP_loader,total=len(train_MCP_loader), desc='Epoch [{}/{}]'.format(epoch, epochs - 1))
    for batch in loop:
        model.train()
        # initialize calculated gradients (from prev step)
        optim.zero_grad()
        # pull all tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        # process
        outputs = model(input_ids, 
                        attention_mask=attention_mask,
                        labels=labels)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optim.step()

        train_loss = train_loss + loss.item()
        loop.set_postfix(loss=loss.item())

    val_loop = tqdm(val_MCP_loader,total=len(val_MCP_loader), desc='Validation')
    for idx, batch in enumerate(val_loop):
      model.eval()
      input_ids = batch['input_ids'].to(device)
      attention_mask = batch['attention_mask'].to(device)
      labels = batch['labels'].to(device)
      with torch.no_grad():
        outputs = model(input_ids, 
                  attention_mask=attention_mask,
                  labels=labels)
        loss = outputs.loss
        val_loss = val_loss + loss.item()

    avg_train_loss.append(train_loss/ len(train_MCP_loader))
    avg_val_loss.append(val_loss/ len(val_MCP_loader))

torch.save(model.state_dict(),"/content/MSP_model.pt")

train_perplexity_list , val_perplexity_list = calculate_perplexity(avg_train_loss,avg_val_loss)

"""##### Graphs"""

N = 6
plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0, N), train_perplexity_list, label="train perplexity")
plt.plot(np.arange(0, N), val_perplexity_list, label="validation perplexity")

plt.title("Training & Validation Perplexity")
plt.xlabel("Epochs")
plt.ylabel("Perplexity")
plt.legend(loc="upper right")
plt.figure(figsize = (20,20))
plt.show()

"""### Inference"""

from doduo.model import BertForMultiOutputClassification, BertMultiPairPooler
from doduo.dataset import TURLColTypeTablewiseDataset , collate_fn
from torch.utils.data import DataLoader, RandomSampler
from sklearn.metrics import multilabel_confusion_matrix
from sklearn.metrics import confusion_matrix, f1_score
import numpy as np
import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)

"""#### Inference Functions"""

def f1_score_multilabel(true_list, pred_list):
    conf_mat = multilabel_confusion_matrix(np.array(true_list),
                                           np.array(pred_list))
    agg_conf_mat = conf_mat.sum(axis=0)
    # Note: Pos F1
    # [[TN FP], [FN, TP]] if we consider 1 as the positive class
    p = agg_conf_mat[1, 1] / agg_conf_mat[1, :].sum()
    r = agg_conf_mat[1, 1] / agg_conf_mat[:, 1].sum()
    
    micro_f1 = 2 * p * r / (p  + r) if (p + r) > 0 else 0.
    class_p = conf_mat[:, 1, 1] /  conf_mat[:, 1, :].sum(axis=1)
    class_r = conf_mat[:, 1, 1] /  conf_mat[:, :, 1].sum(axis=1)
    class_f1 = np.divide(2 * (class_p * class_r), class_p + class_r,
                         out=np.zeros_like(class_p), where=(class_p + class_r) != 0)
    class_f1 = np.nan_to_num(class_f1)
    macro_f1 = class_f1.mean()
    return (micro_f1, macro_f1, class_f1, conf_mat)



def doduo_inference(test_dataloader,doduo_pretrained):
  '''
  Inference with respect to the test_dataloader and the pretrained model
  '''
  ts_pred_list = []
  ts_true_list = []
  # Test
  for batch_idx, batch in enumerate(test_dataloader):

          # Multi-column
          logits, = doduo_pretrained(batch["data"].T)
          if len(logits.shape) == 2:
              logits = logits.unsqueeze(0)
          cls_indexes = torch.nonzero(
              batch["data"].T == tokenizer.cls_token_id)
          filtered_logits = torch.zeros(cls_indexes.shape[0],
                                        logits.shape[2]).to(device)
          for n in range(cls_indexes.shape[0]):
              i, j = cls_indexes[n]
              logit_n = logits[i, j, :]
              filtered_logits[n] = logit_n
          if "sato" in task:
              ts_pred_list += filtered_logits.argmax(
                  1).cpu().detach().numpy().tolist()
              ts_true_list += batch["label"].cpu().detach().numpy(
              ).tolist()
          elif "turl" in task:
              if "turl-re" in task:  # turl-re-colpair
                  all_preds = (filtered_logits >= math.log(0.5)
                                ).int().detach().cpu().numpy()
                  all_labels = batch["label"].cpu().detach().numpy()
                  idxes = np.where(all_labels > 0)[0]
                  ts_pred_list += all_preds[idxes, :].tolist()
                  ts_true_list += all_labels[idxes, :].tolist()
              elif task == "turl":
                  ts_pred_list += (filtered_logits >= math.log(0.5)
                                    ).int().detach().cpu().tolist()
                  ts_true_list += batch["label"].cpu().detach(
                  ).numpy().tolist()
            
  return ts_true_list , ts_pred_list

"""#### Load Pretrained DODUO model"""

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
task = 'turl'


doduo_pretrained = BertForMultiOutputClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=255,
    output_attentions=False,
    output_hidden_states=False,
)


model_path = '/content/doduo/model/turlturl-re-colpair_mosato_bert_bert-base-uncased-bs16-ml-16__turl-1.00_turl-re-1.00=turl_best_micro_f1.pt'
doduo_pretrained.load_state_dict(torch.load(model_path, map_location=device))

doduo_pretrained.encoder = model.bert.encoder

"""#### Test Dataloader & Inference"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')


filepath = "data/table_col_type_serialized.pkl"
dataset_cls = TURLColTypeTablewiseDataset

batch_size = 16
max_length = 32


test_dataset = dataset_cls(filepath=filepath,
                            split="test",
                            tokenizer=tokenizer,
                            max_length=max_length,
                            multicol_only=False,
                            device=device)
test_dataloader = DataLoader(test_dataset,
                              batch_size=batch_size,
                              collate_fn=collate_fn)

tokenizer.decode(test_dataset[1].get('data'))

doduo_pretrained.to(device)

ts_true_list , ts_pred_list = doduo_inference(test_dataloader,doduo_pretrained)
ts_micro_f1, ts_macro_f1, ts_class_f1, ts_conf_mat = f1_score_multilabel(ts_true_list, ts_pred_list)

print('ts_micro_f1 : {}'.format(ts_micro_f1))
print('ts_macro_f1 : {}'.format(ts_macro_f1))
